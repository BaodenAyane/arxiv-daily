---
layout: default
---

## Updated on 2025.06.01
> Usage instructions: [here](./docs/README.md#usage)

## Bandits

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-29**|**COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents**|Arun Verma et.al.|[2505.23720](http://arxiv.org/abs/2505.23720)|null|
|**2025-05-29**|**TRAP: Targeted Redirecting of Agentic Preferences**|Hangoo Kang et.al.|[2505.23518](http://arxiv.org/abs/2505.23518)|null|
|**2025-05-29**|**Emergent Risk Awareness in Rational Agents under Resource Constraints**|Daniel Jarne Ornia et.al.|[2505.23436](http://arxiv.org/abs/2505.23436)|null|
|**2025-05-28**|**Pure Exploration with Infinite Answers**|Riccardo Poiani et.al.|[2505.22473](http://arxiv.org/abs/2505.22473)|null|
|**2025-05-28**|**Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles**|Xiangyu Chang et.al.|[2505.22361](http://arxiv.org/abs/2505.22361)|null|
|**2025-05-28**|**Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection**|Qirun Zeng et.al.|[2505.21938](http://arxiv.org/abs/2505.21938)|null|
|**2025-05-27**|**Faster Rates for Private Adversarial Bandits**|Hilal Asi et.al.|[2505.21790](http://arxiv.org/abs/2505.21790)|null|
|**2025-05-27**|**Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits**|Maoli Liu et.al.|[2505.21393](http://arxiv.org/abs/2505.21393)|null|
|**2025-05-27**|**Improving LLM-based Global Optimization with Search Space Partitioning**|Andrej Schwanke et.al.|[2505.21372](http://arxiv.org/abs/2505.21372)|null|
|**2025-05-27**|**Counterfactual Multi-player Bandits for Explainable Recommendation Diversification**|Yansen Zhang et.al.|[2505.21165](http://arxiv.org/abs/2505.21165)|**[link](https://github.com/forrest-stone/cmb)**|
|**2025-05-22**|**Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype**|Nikola Tankovic et.al.|[2505.16918](http://arxiv.org/abs/2505.16918)|null|
|**2025-05-22**|**Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions**|Marc Brooks et.al.|[2505.16311](http://arxiv.org/abs/2505.16311)|null|
|**2025-05-21**|**Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems**|Long Wanga et.al.|[2505.15862](http://arxiv.org/abs/2505.15862)|null|
|**2025-05-21**|**Improving planning and MBRL with temporally-extended actions**|Palash Chatterjee et.al.|[2505.15754](http://arxiv.org/abs/2505.15754)|null|
|**2025-05-21**|**Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima**|Lan V. Truong et.al.|[2505.15643](http://arxiv.org/abs/2505.15643)|null|
|**2025-05-21**|**Human in the Loop Adaptive Optimization for Improved Time Series Forecasting**|Malik Tiomoko et.al.|[2505.15354](http://arxiv.org/abs/2505.15354)|**[link](https://github.com/posttraining/post_training)**|
|**2025-05-21**|**R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization**|Yuante Li et.al.|[2505.15155](http://arxiv.org/abs/2505.15155)|**[link](https://github.com/microsoft/qlib)**|
|**2025-05-21**|**BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms**|Yunlong Hou et.al.|[2505.15141](http://arxiv.org/abs/2505.15141)|null|
|**2025-05-21**|**In-Domain African Languages Translation Using LLMs and Multi-armed Bandits**|Pratik Rakesh Singh et.al.|[2505.15069](http://arxiv.org/abs/2505.15069)|null|
|**2025-05-20**|**Self-Evolving Curriculum for LLM Reasoning**|Xiaoyin Chen et.al.|[2505.14970](http://arxiv.org/abs/2505.14970)|null|
|**2025-05-19**|**Multi-Armed Bandits Meet Large Language Models**|Djallel Bouneffouf et.al.|[2505.13355](http://arxiv.org/abs/2505.13355)|null|
|**2025-05-19**|**Learning Driven Elastic Task Multi-Connectivity Immersive Computing Systems**|Babak Badnava et.al.|[2505.13331](http://arxiv.org/abs/2505.13331)|null|
|**2025-05-19**|**Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities**|Lili Zhang et.al.|[2505.13195](http://arxiv.org/abs/2505.13195)|null|
|**2025-05-18**|**Not All Documents Are What You Need for Extracting Instruction Tuning Data**|Chi Zhang et.al.|[2505.12250](http://arxiv.org/abs/2505.12250)|null|
|**2025-05-20**|**Thompson Sampling-like Algorithms for Stochastic Rising Bandits**|Marco Fiandri et.al.|[2505.12092](http://arxiv.org/abs/2505.12092)|null|
|**2025-05-17**|**Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment**|Siliang Zeng et.al.|[2505.11821](http://arxiv.org/abs/2505.11821)|null|
|**2025-05-15**|**Assumption-lean weak limits and tests for two-stage adaptive experiments**|Ziang Niu et.al.|[2505.10747](http://arxiv.org/abs/2505.10747)|null|
|**2025-05-15**|**Asymptotically-Optimal Gaussian Bandits with Side Observations**|Alexia Atsidakou et.al.|[2505.10698](http://arxiv.org/abs/2505.10698)|null|
|**2025-05-15**|**Batched Nonparametric Bandits via k-Nearest Neighbor UCB**|Sakshi Arya et.al.|[2505.10498](http://arxiv.org/abs/2505.10498)|null|
|**2025-05-15**|**Near Optimal Best Arm Identification for Clustered Bandits**|Yash et.al.|[2505.10147](http://arxiv.org/abs/2505.10147)|null|
|**2025-05-14**|**Communication-Efficient Distributed Online Nonconvex Optimization with Time-Varying Constraints**|Kunpeng Zhang et.al.|[2505.08592](http://arxiv.org/abs/2505.08592)|null|
|**2025-05-12**|**Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making**|Prakhar Godara et.al.|[2505.08049](http://arxiv.org/abs/2505.08049)|null|
|**2025-05-12**|**Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience**|Seyed Bagher Hashemi Natanzi et.al.|[2505.08032](http://arxiv.org/abs/2505.08032)|null|
|**2025-05-12**|**LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning**|Xiaotian Lin et.al.|[2505.07437](http://arxiv.org/abs/2505.07437)|**[link](https://github.com/HKUSTDial/LEAD)**|
|**2025-05-12**|**Generalization Bounds and Stopping Rules for Learning with Self-Selected Data**|Julian Rodemann et.al.|[2505.07367](http://arxiv.org/abs/2505.07367)|null|
|**2025-05-12**|**Online Episodic Convex Reinforcement Learning**|Bianca Marin Moreno et.al.|[2505.07303](http://arxiv.org/abs/2505.07303)|null|
|**2025-05-13**|**Coordinated Spatial Reuse Scheduling With Machine Learning in IEEE 802.11 MAPC Networks**|Maksymilian Wojnar et.al.|[2505.07278](http://arxiv.org/abs/2505.07278)|**[link](https://github.com/ml4wifi-devs/csr)**|
|**2025-05-12**|**Adaptive, Robust and Scalable Bayesian Filtering for Online Learning**|Gerardo Duran-Martin et.al.|[2505.07267](http://arxiv.org/abs/2505.07267)|null|
|**2025-05-11**|**Constrained Online Decision-Making with Density Estimation Oracles**|Haichen Hu et.al.|[2505.07101](http://arxiv.org/abs/2505.07101)|null|
|**2025-05-11**|**Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users**|Julian Rosenberger et.al.|[2505.07100](http://arxiv.org/abs/2505.07100)|null|
|**2025-05-08**|**Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning**|Amir Rezaei Balef et.al.|[2505.05226](http://arxiv.org/abs/2505.05226)|null|
|**2025-05-08**|**Bandit Max-Min Fair Allocation**|Tsubasa Harada et.al.|[2505.05169](http://arxiv.org/abs/2505.05169)|null|
|**2025-05-08**|**Sample Complexity of Identifying the Nonredundancy of Nontransitive Games in Dueling Bandits**|Shang Lu et.al.|[2505.05014](http://arxiv.org/abs/2505.05014)|null|
|**2025-05-07**|**Estimating Causal Effects in Networks with Cluster-Based Bandits**|Ahmed Sayeed Faruk et.al.|[2505.04200](http://arxiv.org/abs/2505.04200)|null|
|**2025-05-05**|**CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation**|Cairong Yan et.al.|[2505.03840](http://arxiv.org/abs/2505.03840)|null|
|**2025-05-06**|**Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation**|Max Qiushi Lin et.al.|[2505.03155](http://arxiv.org/abs/2505.03155)|null|
|**2025-05-05**|**Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints**|Shubham Vaishnav et.al.|[2505.02640](http://arxiv.org/abs/2505.02640)|null|
|**2025-05-05**|**Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret**|Bingshan Hu et.al.|[2505.02383](http://arxiv.org/abs/2505.02383)|null|
|**2025-05-04**|**Neural Logistic Bandits**|Seoungbin Bae et.al.|[2505.02069](http://arxiv.org/abs/2505.02069)|null|
|**2025-05-03**|**Bayesian learning of the optimal action-value function in a Markov decision process**|Jiaqi Guo et.al.|[2505.01859](http://arxiv.org/abs/2505.01859)|null|
|**2025-05-02**|**DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects**|Shu Tamano et.al.|[2505.00961](http://arxiv.org/abs/2505.00961)|null|
|**2025-05-04**|**Distributed Online Randomized Gradient-Free optimization with Compressed Communication**|Longkang Zhu et.al.|[2504.21693](http://arxiv.org/abs/2504.21693)|null|
|**2025-04-29**|**Does Feedback Help in Bandits with Arm Erasures?**|Merve Karakas et.al.|[2504.20894](http://arxiv.org/abs/2504.20894)|null|
|**2025-04-30**|**Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms**|Meltem Tatlı et.al.|[2504.20877](http://arxiv.org/abs/2504.20877)|null|
|**2025-04-25**|**Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence**|Philipp Kuehn et.al.|[2504.18375](http://arxiv.org/abs/2504.18375)|null|
|**2025-04-25**|**Joint Resource Estimation and Trajectory Optimization for eVTOL-involved CR network: A Monte Carlo Tree Search-based Approach**|Kai Xiong et.al.|[2504.18031](http://arxiv.org/abs/2504.18031)|null|
|**2025-04-24**|**ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders**|Zongliang Ji et.al.|[2504.17277](http://arxiv.org/abs/2504.17277)|null|
|**2025-04-23**|**Natural Policy Gradient for Average Reward Non-Stationary RL**|Neharika Jali et.al.|[2504.16415](http://arxiv.org/abs/2504.16415)|null|
|**2025-04-23**|**The Safety-Privacy Tradeoff in Linear Bandits**|Arghavan Zibaie et.al.|[2504.16371](http://arxiv.org/abs/2504.16371)|null|
|**2025-04-24**|**One-Point Sampling for Distributed Bandit Convex Optimization with Time-Varying Constraints**|Kunpeng Zhang et.al.|[2504.16211](http://arxiv.org/abs/2504.16211)|null|
|**2025-04-22**|**LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities**|Thomas Schmied et.al.|[2504.16078](http://arxiv.org/abs/2504.16078)|null|
|**2025-04-22**|**Fusing Reward and Dueling Feedback in Stochastic Bandits**|Xuchuang Wang et.al.|[2504.15812](http://arxiv.org/abs/2504.15812)|null|
|**2025-04-20**|**Sensor Scheduling in Intrusion Detection Games with Uncertain Payoffs**|Jayanth Bhargav et.al.|[2504.14725](http://arxiv.org/abs/2504.14725)|null|
|**2025-04-19**|**Exploring Pseudo-Token Approaches in Transformer Neural Processes**|Jose Lara-Rangel et.al.|[2504.14416](http://arxiv.org/abs/2504.14416)|null|
|**2025-04-18**|**Access Probability Optimization in RACH: A Multi-Armed Bandits Approach**|Ahmed O. Elmeligy et.al.|[2504.14085](http://arxiv.org/abs/2504.14085)|null|
|**2025-04-16**|**Neural Contextual Bandits Under Delayed Feedback Constraints**|Mohammadali Moghimi et.al.|[2504.12086](http://arxiv.org/abs/2504.12086)|null|
|**2025-04-16**|**Active Human Feedback Collection via Neural Contextual Dueling Bandits**|Arun Verma et.al.|[2504.12016](http://arxiv.org/abs/2504.12016)|null|
|**2025-04-16**|**On the Problem of Best Arm Retention**|Houshuang Chen et.al.|[2504.11866](http://arxiv.org/abs/2504.11866)|null|
|**2025-04-15**|**Learning-Based User Association for MmWave Vehicular Networks With Kernelized Contextual Bandits**|Xiaoyang He et.al.|[2504.10959](http://arxiv.org/abs/2504.10959)|null|
|**2025-04-14**|**Improving Upon the generalized c-mu rule: a Whittle approach**|Zhouzi Li et.al.|[2504.10622](http://arxiv.org/abs/2504.10622)|null|
|**2025-04-14**|**LLM-driven Constrained Copy Generation through Iterative Refinement**|Varun Vasudevan et.al.|[2504.10391](http://arxiv.org/abs/2504.10391)|null|
|**2025-04-12**|**Breaking the Lens of the Telescope: Online Relevance Estimation over Large Retrieval Sets**|Mandeep Rathee et.al.|[2504.09353](http://arxiv.org/abs/2504.09353)|null|
|**2025-04-12**|**Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning**|Zhiyong Wang et.al.|[2504.09192](http://arxiv.org/abs/2504.09192)|null|
|**2025-04-11**|**Scalable Conflict-free Decision Making with Photons**|Kohei Konaka et.al.|[2504.08331](http://arxiv.org/abs/2504.08331)|null|
|**2025-04-11**|**Influential Bandits: Pulling an Arm May Change the Environment**|Ryoma Sato et.al.|[2504.08200](http://arxiv.org/abs/2504.08200)|null|
|**2025-04-09**|**Follow-the-Perturbed-Leader Achieves Best-of-Both-Worlds for the m-Set Semi-Bandit Problems**|Jingxin Zhan et.al.|[2504.07307](http://arxiv.org/abs/2504.07307)|null|
|**2025-04-09**|**Regret Bounds for Robust Online Decision Making**|Alexander Appel et.al.|[2504.06820](http://arxiv.org/abs/2504.06820)|null|
|**2025-04-08**|**Context-aware Rate Adaptation for Predictive Flying Networks using Contextual Bandits**|Ruben Queiros et.al.|[2504.05964](http://arxiv.org/abs/2504.05964)|null|
|**2025-04-07**|**Age-of-information minimization under energy harvesting and non-stationary environment**|Akanksha Jaiswal et.al.|[2504.04916](http://arxiv.org/abs/2504.04916)|null|
|**2025-04-06**|**A Classification View on Meta Learning Bandits**|Mirco Mutti et.al.|[2504.04505](http://arxiv.org/abs/2504.04505)|null|
|**2025-04-04**|**An Exploration-free Method for a Linear Stochastic Bandit Driven by a Linear Gaussian Dynamical System**|Jonathan Gornet et.al.|[2504.03926](http://arxiv.org/abs/2504.03926)|null|
|**2025-04-04**|**Throughput-Optimal Random Access: A Queueing-Theoretical Analysis for Learning-Based Access Design**|Xinran Zhao et.al.|[2504.03178](http://arxiv.org/abs/2504.03178)|null|
|**2025-04-03**|**Prompt Optimization with Logged Bandit Data**|Haruka Kiyohara et.al.|[2504.02646](http://arxiv.org/abs/2504.02646)|null|
|**2025-04-03**|**Quantum Lipschitz Bandits**|Bongsoo Yi et.al.|[2504.02251](http://arxiv.org/abs/2504.02251)|null|
|**2025-04-02**|**Ordering-based Conditions for Global Convergence of Policy Gradient Methods**|Jincheng Mei et.al.|[2504.02130](http://arxiv.org/abs/2504.02130)|null|
|**2025-04-02**|**Antithetic Sampling for Top-k Shapley Identification**|Patrick Kolpaczki et.al.|[2504.02019](http://arxiv.org/abs/2504.02019)|null|
|**2025-04-01**|**Efficient Near-Optimal Algorithm for Online Shortest Paths in Directed Acyclic Graphs with Bandit Feedback Against Adaptive Adversaries**|Arnab Maiti et.al.|[2504.00461](http://arxiv.org/abs/2504.00461)|null|
|**2025-03-28**|**Reinforcement Learning for Machine Learning Model Deployment: Evaluating Multi-Armed Bandits in ML Ops Environments**|S. Aaron McClendon et.al.|[2503.22595](http://arxiv.org/abs/2503.22595)|null|
|**2025-03-27**|**Distributed Forgetting-factor Regret-based Online Optimization over Undirected Connected Networks**|Lipo Mo et.al.|[2503.21498](http://arxiv.org/abs/2503.21498)|null|
|**2025-03-26**|**Competitive Multi-armed Bandit Games for Resource Sharing**|Hongbo Li et.al.|[2503.20975](http://arxiv.org/abs/2503.20975)|null|
|**2025-03-26**|**Reinforcement Learning for Efficient Toxicity Detection in Competitive Online Video Games**|Jacob Morrier et.al.|[2503.20968](http://arxiv.org/abs/2503.20968)|null|
|**2025-03-25**|**Approximating Opaque Top-k Queries**|Jiwon Chang et.al.|[2503.20119](http://arxiv.org/abs/2503.20119)|null|
|**2025-03-25**|**Capacity-Constrained Online Learning with Delays: Scheduling Frameworks and Regret Trade-offs**|Alexander Ryabchenko et.al.|[2503.19856](http://arxiv.org/abs/2503.19856)|null|
|**2025-03-25**|**Causal Bayesian Optimization with Unknown Graphs**|Jean Durand et.al.|[2503.19554](http://arxiv.org/abs/2503.19554)|null|
|**2025-03-26**|**One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in RLHF**|Xin Cai et.al.|[2503.19523](http://arxiv.org/abs/2503.19523)|null|
|**2025-03-25**|**Integrating Prefetcher Selection with Dynamic Request Allocation Improves Prefetching Efficiency**|Mengming Li et.al.|[2503.19390](http://arxiv.org/abs/2503.19390)|null|
|**2025-03-23**|**CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning**|Yexin Li et.al.|[2503.18980](http://arxiv.org/abs/2503.18980)|null|
|**2025-03-23**|**Beam Scheduling in Millimeter Wave Networks Using the Whittle Index**|Mandar R. Nalavade et.al.|[2503.18133](http://arxiv.org/abs/2503.18133)|null|
|**2025-03-22**|**MultiScale Contextual Bandits for Long Term Objectives**|Richa Rastogi et.al.|[2503.17674](http://arxiv.org/abs/2503.17674)|null|
|**2025-03-21**|**Sparse Additive Contextual Bandits: A Nonparametric Approach for Online Decision-making with High-dimensional Covariates**|Wenjia Wang et.al.|[2503.16941](http://arxiv.org/abs/2503.16941)|null|
|**2025-03-20**|**NeuroSep-CP-LCB: A Deep Learning-based Contextual Multi-armed Bandit Algorithm with Uncertainty Quantification for Early Sepsis Prediction**|Anni Zhou et.al.|[2503.16708](http://arxiv.org/abs/2503.16708)|null|
|**2025-03-20**|**Sparse Nonparametric Contextual Bandits**|Hamish Flynn et.al.|[2503.16382](http://arxiv.org/abs/2503.16382)|null|
|**2025-03-20**|**Learn to Bid as a Price-Maker Wind Power Producer**|Shobhit Singhal et.al.|[2503.16107](http://arxiv.org/abs/2503.16107)|null|
|**2025-03-20**|**Information maximization for a broad variety of multi-armed bandit games**|Alex Barbier-Chebbah et.al.|[2503.15962](http://arxiv.org/abs/2503.15962)|null|
|**2025-03-19**|**Performance-bounded Online Ensemble Learning Method Based on Multi-armed bandits and Its Applications in Real-time Safety Assessment**|Songqiao Hu et.al.|[2503.15581](http://arxiv.org/abs/2503.15581)|**[link](https://github.com/liuzy0708/Awesome_OL)**|
|**2025-03-19**|**A New Benchmark for Online Learning with Budget-Balancing Constraints**|Mark Braverman et.al.|[2503.14796](http://arxiv.org/abs/2503.14796)|null|
|**2025-03-18**|**Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis Prediction with Uncertainty Quantification using Conformal Prediction**|Anni Zhou et.al.|[2503.14663](http://arxiv.org/abs/2503.14663)|null|
|**2025-03-17**|**MetaScale: Test-Time Scaling with Evolving Meta-Thoughts**|Qin Liu et.al.|[2503.13447](http://arxiv.org/abs/2503.13447)|null|
|**2025-03-17**|**PAUSE: Low-Latency and Privacy-Aware Active User Selection for Federated Learning**|Ori Peleg et.al.|[2503.13173](http://arxiv.org/abs/2503.13173)|**[link](https://github.com/oritalp/PAUSE)**|
|**2025-03-15**|**Bi-Criteria Optimization for Combinatorial Bandits: Sublinear Regret and Constraint Violation under Bandit Feedback**|Vaneet Aggarwal et.al.|[2503.12285](http://arxiv.org/abs/2503.12285)|null|
|**2025-03-15**|**Variance-Dependent Regret Lower Bounds for Contextual Bandits**|Jiafan He et.al.|[2503.12020](http://arxiv.org/abs/2503.12020)|null|
|**2025-03-15**|**Automation and Feature Selection Enhancement with Reinforcement Learning (RL)**|Sumana Sanyasipura Nagaraju et.al.|[2503.11991](http://arxiv.org/abs/2503.11991)|null|
|**2025-03-18**|**Clustering Items through Bandit Feedback: Finding the Right Feature out of Many**|Maximilian Graf et.al.|[2503.11209](http://arxiv.org/abs/2503.11209)|null|
|**2025-03-13**|**Exploiting Concavity Information in Gaussian Process Contextual Bandit Optimization**|Kevin Li et.al.|[2503.10836](http://arxiv.org/abs/2503.10836)|null|
|**2025-03-14**|**Multi-objective Good Arm Identification with Bandit Feedback**|Xuanke Jiang et.al.|[2503.10386](http://arxiv.org/abs/2503.10386)|**[link](https://github.com/2015211217/MultiThresholdBandit)**|
|**2025-03-13**|**HyperArm Bandit Optimization: A Novel approach to Hyperparameter Optimization and an Analysis of Bandit Algorithms in Stochastic and Adversarial Settings**|Samih Karroum et.al.|[2503.10282](http://arxiv.org/abs/2503.10282)|null|
|**2025-03-12**|**Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping**|Guangyi Liu et.al.|[2503.09755](http://arxiv.org/abs/2503.09755)|null|
|**2025-03-11**|**Multiplayer Information Asymmetric Contextual Bandits**|William Chang et.al.|[2503.08961](http://arxiv.org/abs/2503.08961)|null|
|**2025-03-11**|**Beam Selection in ISAC using Contextual Bandit with Multi-modal Transformer and Transfer Learning**|Mohammad Farzanullah et.al.|[2503.08937](http://arxiv.org/abs/2503.08937)|null|
|**2025-03-11**|**Risk-sensitive Bandits: Arm Mixture Optimality and Regret-efficient Algorithms**|Meltem Tatlı et.al.|[2503.08896](http://arxiv.org/abs/2503.08896)|**[link](https://github.com/MeltemTatli/Risk-sensitive-Bandits-Arm-Mixture-Optimality)**|
|**2025-03-11**|**Imitation Learning of Correlated Policies in Stackelberg Games**|Kunag-Da Wang et.al.|[2503.08883](http://arxiv.org/abs/2503.08883)|null|
|**2025-03-11**|**Locally Private Nonparametric Contextual Multi-armed Bandits**|Yuheng Ma et.al.|[2503.08098](http://arxiv.org/abs/2503.08098)|**[link](https://github.com/Karlmyh/LDP-Contextual-MAB)**|
|**2025-03-11**|**Multiplayer Information Asymmetric Bandits in Metric Spaces**|William Chang et.al.|[2503.08004](http://arxiv.org/abs/2503.08004)|null|
|**2025-03-10**|**Cost-Aware Optimal Pairwise Pure Exploration**|Di Wu et.al.|[2503.07877](http://arxiv.org/abs/2503.07877)|null|
|**2025-03-10**|**Pure Exploration with Feedback Graphs**|Alessio Russo et.al.|[2503.07824](http://arxiv.org/abs/2503.07824)|**[link](https://github.com/rssalessio/Pure-Exploration-with-Feedback-Graphs)**|
|**2025-03-10**|**Graph-Dependent Regret Bounds in Multi-Armed Bandits with Interference**|Fateme Jamshidi et.al.|[2503.07555](http://arxiv.org/abs/2503.07555)|null|
|**2025-03-08**|**ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning**|Mingqi Yuan et.al.|[2503.06101](http://arxiv.org/abs/2503.06101)|null|
|**2025-03-07**|**On Mitigating Affinity Bias through Bandits with Evolving Biased Feedback**|Matthew Faw et.al.|[2503.05662](http://arxiv.org/abs/2503.05662)|null|
|**2025-03-07**|**Empirical Bound Information-Directed Sampling for Norm-Agnostic Bandits**|Piotr M. Suder et.al.|[2503.05098](http://arxiv.org/abs/2503.05098)|null|
|**2025-03-06**|**A characterization of sample adaptivity in UCB data**|Yilun Chen et.al.|[2503.04855](http://arxiv.org/abs/2503.04855)|null|
|**2025-03-06**|**Leveraging priors on distribution functions for multi-arm bandits**|Sumit Vashishtha et.al.|[2503.04518](http://arxiv.org/abs/2503.04518)|null|
|**2025-03-06**|**Greedy Algorithm for Structured Bandits: A Sharp Characterization of Asymptotic Success / Failure**|Aleksandrs Slivkins et.al.|[2503.04010](http://arxiv.org/abs/2503.04010)|null|
|**2025-03-04**|**Quantum Non-Linear Bandit Optimization**|Zakaria Shams Siam et.al.|[2503.03023](http://arxiv.org/abs/2503.03023)|null|
|**2025-03-04**|**A Theoretical Model for Grit in Pursuing Ambitious Ends**|Avrim Blum et.al.|[2503.02952](http://arxiv.org/abs/2503.02952)|null|
|**2025-03-04**|**Clustered KL-barycenter design for policy evaluation**|Simon Weissmann et.al.|[2503.02735](http://arxiv.org/abs/2503.02735)|null|
|**2025-03-04**|**Tight Gap-Dependent Memory-Regret Trade-Off for Single-Pass Streaming Stochastic Multi-Armed Bandits**|Zichun Ye et.al.|[2503.02428](http://arxiv.org/abs/2503.02428)|null|
|**2025-03-03**|**Constrained Linear Thompson Sampling**|Aditya Gangrade et.al.|[2503.02043](http://arxiv.org/abs/2503.02043)|null|
|**2025-03-01**|**Reinforcement learning with combinatorial actions for coupled restless bandits**|Lily Xu et.al.|[2503.01919](http://arxiv.org/abs/2503.01919)|null|
|**2025-03-03**|**MAB-Based Channel Scheduling for Asynchronous Federated Learning in Non-Stationary Environments**|Zhiyin Li et.al.|[2503.01324](http://arxiv.org/abs/2503.01324)|null|
|**2025-03-03**|**Architectural and Inferential Inductive Biases For Exchangeable Sequence Modeling**|Daksh Mittal et.al.|[2503.01215](http://arxiv.org/abs/2503.01215)|**[link](https://github.com/namkoong-lab/inductive-biases-exchangeable-sequence)**|
|**2025-03-03**|**Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers**|Rin Ashizawa et.al.|[2503.01163](http://arxiv.org/abs/2503.01163)|null|
|**2025-03-02**|**Parameter-Adaptive Dynamic Pricing**|Xueping Gong et.al.|[2503.00929](http://arxiv.org/abs/2503.00929)|null|
|**2025-03-01**|**Semi-Parametric Batched Global Multi-Armed Bandits with Covariates**|Sakshi Arya et.al.|[2503.00565](http://arxiv.org/abs/2503.00565)|null|
|**2025-03-01**|**Heavy-Tailed Linear Bandits: Huber Regression with One-Pass Update**|Jing Wang et.al.|[2503.00419](http://arxiv.org/abs/2503.00419)|null|
|**2025-02-24**|**Linear Bandits on Ellipsoids: Minimax Optimal Algorithms**|Raymond Zhang et.al.|[2502.17175](http://arxiv.org/abs/2502.17175)|null|
|**2025-02-20**|**Contextual Linear Bandits with Delay as Payoff**|Mengxiao Zhang et.al.|[2502.12528](http://arxiv.org/abs/2502.12528)|null|
|**2025-02-13**|**When and why randomised exploration works (in linear bandits)**|Marc Abeille et.al.|[2502.08870](http://arxiv.org/abs/2502.08870)|null|
|**2025-02-12**|**Cascading Bandits Robust to Adversarial Corruptions**|Jize Xie et.al.|[2502.08077](http://arxiv.org/abs/2502.08077)|null|
|**2025-02-11**|**A Near-optimal, Scalable and Corruption-tolerant Framework for Stochastic Bandits: From Single-Agent to Multi-Agent and Beyond**|Zicheng Hu et.al.|[2502.07514](http://arxiv.org/abs/2502.07514)|null|
|**2025-02-11**|**Bandit Optimal Transport**|Lorenzo Croissant et.al.|[2502.07397](http://arxiv.org/abs/2502.07397)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

