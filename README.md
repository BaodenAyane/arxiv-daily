## Updated on 2025.03.21
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#bandits>Bandits</a></li>
  </ol>
</details>

## Bandits

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-03-19**|**A New Benchmark for Online Learning with Budget-Balancing Constraints**|Mark Braverman et.al.|[2503.14796](http://arxiv.org/abs/2503.14796)|null|
|**2025-03-18**|**Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis Prediction with Uncertainty Quantification using Conformal Prediction**|Anni Zhou et.al.|[2503.14663](http://arxiv.org/abs/2503.14663)|null|
|**2025-03-17**|**MetaScale: Test-Time Scaling with Evolving Meta-Thoughts**|Qin Liu et.al.|[2503.13447](http://arxiv.org/abs/2503.13447)|null|
|**2025-03-17**|**PAUSE: Low-Latency and Privacy-Aware Active User Selection for Federated Learning**|Ori Peleg et.al.|[2503.13173](http://arxiv.org/abs/2503.13173)|null|
|**2025-03-15**|**Bi-Criteria Optimization for Combinatorial Bandits: Sublinear Regret and Constraint Violation under Bandit Feedback**|Vaneet Aggarwal et.al.|[2503.12285](http://arxiv.org/abs/2503.12285)|null|
|**2025-03-15**|**Variance-Dependent Regret Lower Bounds for Contextual Bandits**|Jiafan He et.al.|[2503.12020](http://arxiv.org/abs/2503.12020)|null|
|**2025-03-15**|**Automation and Feature Selection Enhancement with Reinforcement Learning (RL)**|Sumana Sanyasipura Nagaraju et.al.|[2503.11991](http://arxiv.org/abs/2503.11991)|null|
|**2025-03-18**|**Clustering Items through Bandit Feedback: Finding the Right Feature out of Many**|Maximilian Graf et.al.|[2503.11209](http://arxiv.org/abs/2503.11209)|null|
|**2025-03-13**|**Exploiting Concavity Information in Gaussian Process Contextual Bandit Optimization**|Kevin Li et.al.|[2503.10836](http://arxiv.org/abs/2503.10836)|null|
|**2025-03-14**|**Multi-objective Good Arm Identification with Bandit Feedback**|Xuanke Jiang et.al.|[2503.10386](http://arxiv.org/abs/2503.10386)|**[link](https://github.com/2015211217/MultiThresholdBandit)**|
|**2025-03-13**|**HyperArm Bandit Optimization: A Novel approach to Hyperparameter Optimization and an Analysis of Bandit Algorithms in Stochastic and Adversarial Settings**|Samih Karroum et.al.|[2503.10282](http://arxiv.org/abs/2503.10282)|null|
|**2025-03-12**|**Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping**|Guangyi Liu et.al.|[2503.09755](http://arxiv.org/abs/2503.09755)|null|
|**2025-03-11**|**Multiplayer Information Asymmetric Contextual Bandits**|William Chang et.al.|[2503.08961](http://arxiv.org/abs/2503.08961)|null|
|**2025-03-11**|**Beam Selection in ISAC using Contextual Bandit with Multi-modal Transformer and Transfer Learning**|Mohammad Farzanullah et.al.|[2503.08937](http://arxiv.org/abs/2503.08937)|null|
|**2025-03-11**|**Risk-sensitive Bandits: Arm Mixture Optimality and Regret-efficient Algorithms**|Meltem TatlÄ± et.al.|[2503.08896](http://arxiv.org/abs/2503.08896)|**[link](https://github.com/MeltemTatli/Risk-sensitive-Bandits-Arm-Mixture-Optimality)**|
|**2025-03-11**|**Imitation Learning of Correlated Policies in Stackelberg Games**|Kunag-Da Wang et.al.|[2503.08883](http://arxiv.org/abs/2503.08883)|null|
|**2025-03-11**|**Locally Private Nonparametric Contextual Multi-armed Bandits**|Yuheng Ma et.al.|[2503.08098](http://arxiv.org/abs/2503.08098)|**[link](https://github.com/Karlmyh/LDP-Contextual-MAB)**|
|**2025-03-11**|**Multiplayer Information Asymmetric Bandits in Metric Spaces**|William Chang et.al.|[2503.08004](http://arxiv.org/abs/2503.08004)|null|
|**2025-03-10**|**Cost-Aware Optimal Pairwise Pure Exploration**|Di Wu et.al.|[2503.07877](http://arxiv.org/abs/2503.07877)|null|
|**2025-03-10**|**Pure Exploration with Feedback Graphs**|Alessio Russo et.al.|[2503.07824](http://arxiv.org/abs/2503.07824)|**[link](https://github.com/rssalessio/Pure-Exploration-with-Feedback-Graphs)**|
|**2025-03-10**|**Graph-Dependent Regret Bounds in Multi-Armed Bandits with Interference**|Fateme Jamshidi et.al.|[2503.07555](http://arxiv.org/abs/2503.07555)|null|
|**2025-03-08**|**ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning**|Mingqi Yuan et.al.|[2503.06101](http://arxiv.org/abs/2503.06101)|null|
|**2025-03-07**|**On Mitigating Affinity Bias through Bandits with Evolving Biased Feedback**|Matthew Faw et.al.|[2503.05662](http://arxiv.org/abs/2503.05662)|null|
|**2025-03-07**|**Empirical Bound Information-Directed Sampling for Norm-Agnostic Bandits**|Piotr M. Suder et.al.|[2503.05098](http://arxiv.org/abs/2503.05098)|null|
|**2025-03-06**|**A characterization of sample adaptivity in UCB data**|Yilun Chen et.al.|[2503.04855](http://arxiv.org/abs/2503.04855)|null|
|**2025-03-06**|**Leveraging priors on distribution functions for multi-arm bandits**|Sumit Vashishtha et.al.|[2503.04518](http://arxiv.org/abs/2503.04518)|null|
|**2025-03-06**|**Greedy Algorithm for Structured Bandits: A Sharp Characterization of Asymptotic Success / Failure**|Aleksandrs Slivkins et.al.|[2503.04010](http://arxiv.org/abs/2503.04010)|null|
|**2025-03-04**|**Quantum Non-Linear Bandit Optimization**|Zakaria Shams Siam et.al.|[2503.03023](http://arxiv.org/abs/2503.03023)|null|
|**2025-03-04**|**A Theoretical Model for Grit in Pursuing Ambitious Ends**|Avrim Blum et.al.|[2503.02952](http://arxiv.org/abs/2503.02952)|null|
|**2025-03-04**|**Clustered KL-barycenter design for policy evaluation**|Simon Weissmann et.al.|[2503.02735](http://arxiv.org/abs/2503.02735)|null|
|**2025-03-04**|**Tight Gap-Dependent Memory-Regret Trade-Off for Single-Pass Streaming Stochastic Multi-Armed Bandits**|Zichun Ye et.al.|[2503.02428](http://arxiv.org/abs/2503.02428)|null|
|**2025-03-03**|**Constrained Linear Thompson Sampling**|Aditya Gangrade et.al.|[2503.02043](http://arxiv.org/abs/2503.02043)|null|
|**2025-03-01**|**Reinforcement learning with combinatorial actions for coupled restless bandits**|Lily Xu et.al.|[2503.01919](http://arxiv.org/abs/2503.01919)|null|
|**2025-03-03**|**MAB-Based Channel Scheduling for Asynchronous Federated Learning in Non-Stationary Environments**|Zhiyin Li et.al.|[2503.01324](http://arxiv.org/abs/2503.01324)|null|
|**2025-03-03**|**Architectural and Inferential Inductive Biases For Exchangeable Sequence Modeling**|Daksh Mittal et.al.|[2503.01215](http://arxiv.org/abs/2503.01215)|**[link](https://github.com/namkoong-lab/inductive-biases-exchangeable-sequence)**|
|**2025-03-03**|**Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers**|Rin Ashizawa et.al.|[2503.01163](http://arxiv.org/abs/2503.01163)|null|
|**2025-03-02**|**Parameter-Adaptive Dynamic Pricing**|Xueping Gong et.al.|[2503.00929](http://arxiv.org/abs/2503.00929)|null|
|**2025-03-01**|**Semi-Parametric Batched Global Multi-Armed Bandits with Covariates**|Sakshi Arya et.al.|[2503.00565](http://arxiv.org/abs/2503.00565)|null|
|**2025-03-01**|**Heavy-Tailed Linear Bandits: Huber Regression with One-Pass Update**|Jing Wang et.al.|[2503.00419](http://arxiv.org/abs/2503.00419)|null|
|**2025-02-24**|**Linear Bandits on Ellipsoids: Minimax Optimal Algorithms**|Raymond Zhang et.al.|[2502.17175](http://arxiv.org/abs/2502.17175)|null|
|**2025-02-20**|**Contextual Linear Bandits with Delay as Payoff**|Mengxiao Zhang et.al.|[2502.12528](http://arxiv.org/abs/2502.12528)|null|
|**2025-02-13**|**When and why randomised exploration works (in linear bandits)**|Marc Abeille et.al.|[2502.08870](http://arxiv.org/abs/2502.08870)|null|
|**2025-02-12**|**Cascading Bandits Robust to Adversarial Corruptions**|Jize Xie et.al.|[2502.08077](http://arxiv.org/abs/2502.08077)|null|
|**2025-02-11**|**A Near-optimal, Scalable and Corruption-tolerant Framework for Stochastic Bandits: From Single-Agent to Multi-Agent and Beyond**|Zicheng Hu et.al.|[2502.07514](http://arxiv.org/abs/2502.07514)|null|
|**2025-02-11**|**Bandit Optimal Transport**|Lorenzo Croissant et.al.|[2502.07397](http://arxiv.org/abs/2502.07397)|null|

<p align=right>(<a href=#updated-on-20250321>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

